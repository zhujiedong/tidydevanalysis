[
["index.html", "使用 tidyverse 的方式分析仪器测量的数据 欢迎", " 使用 tidyverse 的方式分析仪器测量的数据 祝介东 北京力高泰科技有限公司 2019-03-21 欢迎 Note: 本文内容仅为我 github 中相关 repo (tidydevanalysis) 的在线预览版本，并不保证及时更新。 本文同样纯属个人打发出差漫漫长夜的作品，并非正式资料，只不过尽量使用了 tidyverse 的方式进行，具体成书原因可参考 photoanalysis。 本文能保证的是只要 github 不关，内容就永远可以在线预览，尽量更新，及时纠错。其余事情概不承诺，参考需谨慎。 另，图片仅仅为使得内容显得正式，为网络盗图，侵删。如果你同样在某些时刻闲的无聊，欢迎给我设计个封面。 "],
["frontmatter.html", "前言", " 前言 目的与用途同 photoanalysis，纯属个人项目，在业余爱好者里面很流行的一个词概括为 “just for fun”，当然能帮助到别人也是一件很开心的事情。 关于问题或建议，如果您有 github 帐号，优先欢迎在 github 提交： github地址 此为个人项目，因此问题或建议请反馈至个人邮箱： zhujiedong@yeah.net 如有其他仪器问题或者仪器购买需求，请使用下面方式与我所在公司联系： 北京力高泰科技有限公司 网址：http://www.ecotek.com.cn 电话：010-51665551 电子邮件：info@ecotek.com.cn 地址：北京市西城区西直门南大街2号成铭大厦A座22F "],
["author.html", "关于作者", " 关于作者 祝介东，北京力高泰科技有限公司 工程师，本文内容为我在售后服务时所住的酒店所作（遍布国内大多数省份），本为我打发出差住酒店漫漫长夜的业余爱好，此部分内容并非公司对我的职位所要求的内容，也并非我公司提供的服务内容，因此属于个人作品，仅供参考，我所在公司对此文内容不负任何责任。 "],
["copyright.html", "版权", " 版权 本文旨在对使用我公司产品的用户提供一个数据分析的参考，本人或所在公司并未从中获取任何利益，内容错误疏漏之处，欢迎指正。本人保留一切权利，禁止一切将本文内容用于商业用途的行为，禁止商业公司使用。 "],
["origin-tidyverse.html", "第 1 章 tidyverse 的起源 1.1 tidyverse 的流程", " 第 1 章 tidyverse 的起源 提到 R，他们创始人的名字估计能讲出来的没那么多，但是 Hadley Wickham 绝对是在他发布 ggplot2 成名后，鼎鼎有名的人物。 书归正传，tidyverse 为一些列符合相同设计哲学、语法和数据结构的包的集合，也就是说只要符合这个定义，都可以算作为 tidyverse，不管是不是 tidyverse 已经收录。 本文内容刻意模仿了 R for Data Science 的风格，如有兴趣，更建议各位直接看原文。 1.1 tidyverse 的流程 很多时候，一言以蔽之后面的内容听上去很概括，实际上效果远不如一副图片更清晰和直观，例如图 1.1 图 1.1: tidyverse 数据分析的流程 实际上这些处理步骤，大家下意识的就会按照这个进行处理，只不过未将其提炼并系统化而已，多数时候我们导入数据后，这些数据并不能立即使用，通俗的称之为“脏”数据，需要对其进行清洁（例如缺失值等），之后可能需要根据已有数据计算部分数据，处理只好后进行可视化建模，最后整理实验结果，不管发表还是报告等，都是交流的过程。 "],
["prerequisites.html", "第 2 章 准备工作", " 第 2 章 准备工作 需要完成 tidyverse 方式的数据分析，必备的工具有1： R Rstudio tidyverse 其中，tidyverse 安装可分为 CRAN 版和 github 版本，这个无需纠结，二者均可： install.packages(&quot;tidyverse&quot;) 或 devtools::install_github(&quot;tidyverse/tidyverse&quot;) Rstudio 为建议必备，其他编辑器也可↩ "],
["readr.html", "第 3 章 tidyverse 数据格式 3.1 属性的差别 3.2 数据类型的解析 3.3 Julian day 的转换 3.4 常用 package 介绍 3.5 readr 包核心函数 3.6 readxl 3.7 二进制文件 3.8 rio–万能的瑞士军刀", " 第 3 章 tidyverse 数据格式 基本的数据导入，这里我们先通过 tidyverse 中的核心包为readr，引入 tidyverse 对数据导入的核心观点，如果熟悉 read.csv 或其他相似的函数，那么使用 readr 包必然也没有障碍，那么我们先用同样的两种方式，导入相同的 csv 文件，观察其差别： library(tidyverse) ## -- Attaching packages ---------------- tidyverse 1.2.1.9000 -- ## √ ggplot2 3.1.0 √ purrr 0.2.5 ## √ tibble 2.0.1 √ dplyr 0.7.8 ## √ tidyr 0.8.2 √ stringr 1.3.1 ## √ readr 1.3.1 √ forcats 0.3.0 ## -- Conflicts ------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() df &lt;- read_csv(&quot;./data/aci_ex.csv&quot;, local = locale(encoding = &quot;latin1&quot;)) ## Parsed with column specification: ## cols( ## .default = col_double(), ## date = col_datetime(format = &quot;&quot;), ## LightAdaptedID = col_character(), ## DarkPulseID = col_character() ## ) ## See spec(...) for full column specifications. df ## # A tibble: 22 x 104 ## obs time elapsed date TIME E A Ci ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.48e9 7372. 2016-10-02 11:05:33 1.48e9 0.00770 29.5 275. ## 2 2 1.48e9 7374. 2016-10-02 11:05:35 1.48e9 0.00781 26.3 239. ## 3 3 1.48e9 7376. 2016-10-02 11:05:37 1.48e9 0.00790 22.8 205. ## 4 4 1.48e9 7378. 2016-10-02 11:05:39 1.48e9 0.00799 18.6 173. ## 5 5 1.48e9 7380. 2016-10-02 11:05:41 1.48e9 0.00806 14.4 140. ## 6 6 1.48e9 7382. 2016-10-02 11:05:43 1.48e9 0.00822 9.73 110. ## 7 7 1.48e9 7384. 2016-10-02 11:05:45 1.48e9 0.00835 5.01 78.8 ## 8 8 1.48e9 7386. 2016-10-02 11:05:47 1.48e9 0.00839 -0.0205 48.5 ## 9 9 1.48e9 7388. 2016-10-02 11:05:49 1.48e9 0.00860 28.8 286. ## 10 10 1.48e9 7390. 2016-10-02 11:05:51 1.48e9 0.00875 34.9 362. ## # ... with 12 more rows, and 96 more variables: Pci &lt;dbl&gt;, Pca &lt;dbl&gt;, ## # gsw &lt;dbl&gt;, gbw &lt;dbl&gt;, gtw &lt;dbl&gt;, gtc &lt;dbl&gt;, Rabs &lt;dbl&gt;, TleafEB &lt;dbl&gt;, ## # TleafCnd &lt;dbl&gt;, SVPleaf &lt;dbl&gt;, RHcham &lt;dbl&gt;, VPcham &lt;dbl&gt;, ## # SVPcham &lt;dbl&gt;, VPDleaf &lt;dbl&gt;, blfa_1 &lt;dbl&gt;, blfa_2 &lt;dbl&gt;, ## # blfa_3 &lt;dbl&gt;, DarkAdaptedID &lt;dbl&gt;, Fo &lt;dbl&gt;, Fm &lt;dbl&gt;, Fv &lt;dbl&gt;, ## # Fv.Fm &lt;dbl&gt;, Adark &lt;dbl&gt;, LightAdaptedID &lt;chr&gt;, Fs &lt;dbl&gt;, Fm. &lt;dbl&gt;, ## # PhiPS2 &lt;dbl&gt;, PS2.1 &lt;dbl&gt;, Qabs_fs &lt;dbl&gt;, Afs &lt;dbl&gt;, ETR &lt;dbl&gt;, ## # Fv..Fm. &lt;dbl&gt;, PhiCO2 &lt;dbl&gt;, NPQ &lt;dbl&gt;, DarkPulseID &lt;chr&gt;, Fo. &lt;dbl&gt;, ## # Fv. &lt;dbl&gt;, qP &lt;dbl&gt;, qN &lt;dbl&gt;, qP_Fo &lt;dbl&gt;, qN_Fo &lt;dbl&gt;, Qin &lt;dbl&gt;, ## # Qabs &lt;dbl&gt;, alpha &lt;dbl&gt;, convert &lt;dbl&gt;, TIME.1 &lt;dbl&gt;, CO2_s &lt;dbl&gt;, ## # CO2_r &lt;dbl&gt;, H2O_s &lt;dbl&gt;, H2O_r &lt;dbl&gt;, Flow &lt;dbl&gt;, Pa &lt;dbl&gt;, ## # `|¤Pcham` &lt;dbl&gt;, Tair &lt;dbl&gt;, Tleaf &lt;dbl&gt;, Tleaf2 &lt;dbl&gt;, ## # Fan_speed &lt;dbl&gt;, Qamb_in &lt;dbl&gt;, Qamb_out &lt;dbl&gt;, Q &lt;dbl&gt;, f_red &lt;dbl&gt;, ## # f_blue &lt;dbl&gt;, f_farred &lt;dbl&gt;, F &lt;dbl&gt;, Q_modavg &lt;dbl&gt;, F_dc &lt;dbl&gt;, ## # Tled &lt;dbl&gt;, TDigital &lt;dbl&gt;, TPreamp &lt;dbl&gt;, TPwrSpy &lt;dbl&gt;, ## # TDrive &lt;dbl&gt;, F_avg &lt;dbl&gt;, dF.dt &lt;dbl&gt;, dF_dc.dt &lt;dbl&gt;, ## # F_dc_avg &lt;dbl&gt;, period &lt;dbl&gt;, DIAG &lt;dbl&gt;, Flow_s &lt;dbl&gt;, Flow_r &lt;dbl&gt;, ## # Txchg &lt;dbl&gt;, Tirga &lt;dbl&gt;, Tchopper &lt;dbl&gt;, Ts &lt;dbl&gt;, Tr &lt;dbl&gt;, ## # CO2_. &lt;dbl&gt;, Desiccant_. &lt;dbl&gt;, Humidifier_. &lt;dbl&gt;, ## # Heatx_setpoint &lt;dbl&gt;, CO2_r_setpoint &lt;dbl&gt;, H2O_r_setpoint &lt;dbl&gt;, ## # SS_s &lt;dbl&gt;, SS_r &lt;dbl&gt;, MatchH2O &lt;dbl&gt;, MatchCO2 &lt;dbl&gt;, ## # MatchValveR &lt;dbl&gt;, MatchValveS &lt;dbl&gt; df &lt;- read.csv(&quot;./data/aci_ex.csv&quot;) head(df) ## obs time elapsed date TIME E A ## 1 1 1475427934 7372.4 20161002 11:05:33 1475427933 0.007696760 29.47670 ## 2 2 1475427936 7374.4 20161002 11:05:35 1475427935 0.007808186 26.31897 ## 3 3 1475427938 7376.4 20161002 11:05:37 1475427937 0.007901756 22.76927 ## 4 4 1475427940 7378.4 20161002 11:05:39 1475427939 0.007987330 18.61361 ## 5 5 1475427942 7380.4 20161002 11:05:41 1475427941 0.008064806 14.43679 ## 6 6 1475427944 7382.4 20161002 11:05:43 1475427943 0.008215388 9.73141 ## Ci Pci Pca gsw gbw gtw gtc ## 1 274.6465 30.69809 38.03046 0.3432676 2.001378 0.4403924 0.2800793 ## 2 239.3628 30.50294 37.78892 0.3481919 2.007294 0.4405757 0.2801836 ## 3 205.2410 30.36986 37.54531 0.3530725 2.002956 0.4405515 0.2801784 ## 4 172.8188 30.21500 37.29877 0.3561105 2.000471 0.4410542 0.2805097 ## 5 140.4173 30.02464 37.05066 0.3602038 2.003897 0.4407989 0.2803362 ## 6 109.8534 29.89953 36.80124 0.3671059 2.000833 0.4404739 0.2801333 ## Rabs TleafEB TleafCnd SVPleaf RHcham VPcham SVPcham VPDleaf ## 1 165.2454 25.96638 24.9904 3.177858 55.40208 1.979484 3.572942 1.198374 ## 2 165.1947 25.96739 24.9941 3.178559 55.40164 1.979585 3.573152 1.198975 ## 3 165.2454 25.97100 24.9860 3.177025 55.39090 1.979457 3.573614 1.197568 ## 4 165.2454 25.96911 24.9840 3.176646 55.38870 1.979506 3.573845 1.197140 ## 5 165.2437 25.97255 24.9865 3.177119 55.37738 1.979323 3.574244 1.197797 ## 6 165.2454 25.97429 24.9857 3.176968 55.36981 1.979192 3.574496 1.197776 ## blfa_1 blfa_2 blfa_3 DarkAdaptedID Fo Fm Fv Fv.Fm Adark ## 1 -0.03479512 0.03906057 3.019639 0 0 0 0 0 -1 ## 2 -0.03494065 0.03922395 3.029814 0 0 0 0 0 -1 ## 3 -0.03483390 0.03910411 3.022352 0 0 0 0 0 -1 ## 4 -0.03477282 0.03903553 3.018079 0 0 0 0 0 -1 ## 5 -0.03485704 0.03913008 3.023969 0 0 0 0 0 -1 ## 6 -0.03478173 0.03904554 3.018702 0 0 0 0 0 -1 ## LightAdaptedID Fs Fm. PhiPS2 PS2.1 Qabs_fs Afs ETR Fv..Fm. PhiCO2 ## 1 - 0 0 0 0.5 0 0 0 0 0.02699408 ## 2 - 0 0 0 0.5 0 0 0 0 0.02683882 ## 3 - 0 0 0 0.5 0 0 0 0 0.02643125 ## 4 - 0 0 0 0.5 0 0 0 0 0.02611003 ## 5 - 0 0 0 0.5 0 0 0 0 0.02588544 ## 6 - 0 0 0 0.5 0 0 0 0 0.02539339 ## NPQ DarkPulseID Fo. Fv. qP qN qP_Fo qN_Fo Qin Qabs alpha ## 1 0 - 0 0 0 0 0 0 1000.060 843.0848 0.8430005 ## 2 0 - 0 0 0 0 0 0 999.706 843.0932 0.8430005 ## 3 0 - 0 0 0 0 0 0 1000.040 842.8319 0.8430005 ## 4 0 - 0 0 0 0 0 0 999.990 843.0924 0.8429996 ## 5 0 - 0 0 0 0 0 0 999.973 843.0839 0.8429996 ## 6 0 - 0 0 0 0 0 0 999.963 843.0755 0.8429996 ## convert TIME.1 CO2_s CO2_r H2O_s H2O_r Flow Pa ## 1 0.1960010 1475427933 367.719 400.026 23.5181 17.1985 599.683 83.9679 ## 2 0.1960010 1475427935 321.236 350.063 23.5197 17.1997 600.208 83.9672 ## 3 0.1960010 1475427937 275.105 300.048 23.5183 17.2012 599.744 83.9667 ## 4 0.1959991 1475427939 229.601 250.048 23.5189 17.2009 600.127 83.9665 ## 5 0.1959991 1475427941 184.130 200.052 23.5173 17.1997 600.164 83.9654 ## 6 0.1959991 1475427943 139.148 150.023 23.5148 17.1972 599.687 83.9670 ## ΔPcham Tair Tleaf Tleaf2 Fan_speed Qamb_in Qamb_out Q ## 1 0.200645 26.9704 25.0272 999.9 9993.12 0 6.93277 1000.10 ## 2 0.199888 26.9714 24.9913 999.9 10035.00 0 6.93277 1000.11 ## 3 0.199957 26.9736 24.9877 999.9 10004.40 0 6.93277 1000.11 ## 4 0.200108 26.9747 25.0090 999.9 9986.88 0 6.93277 999.79 ## 5 0.199135 26.9766 25.0247 999.9 10011.20 0 6.93277 1000.10 ## 6 0.200911 26.9778 25.0009 999.9 9989.38 0 6.93277 1000.09 ## f_red f_blue f_farred F Q_modavg F_dc Tled TDigital ## 1 0.900015 0.0999851 0 2.8427 0 7893.66 38.812 41.812 ## 2 0.899984 0.1000160 0 2.5274 0 7887.49 38.812 41.812 ## 3 0.899984 0.1000160 0 2.6852 0 7878.68 38.812 41.812 ## 4 0.899984 0.1000160 0 2.6955 0 7869.08 38.812 41.812 ## 5 0.900015 0.0999851 0 2.5337 0 7865.47 38.812 41.812 ## 6 0.900015 0.0999851 0 2.3220 0 7856.35 38.812 41.812 ## TPreamp TPwrSpy TDrive F_avg dF.dt dF_dc.dt F_dc_avg period ## 1 40.437 40.625 39.187 2.617227 -0.5654120 -144.1986 7913.660 15 ## 2 40.437 40.625 39.187 2.616973 -0.6446940 -161.5990 7909.170 15 ## 3 40.437 40.625 39.187 2.585404 -0.5877231 -178.2270 7902.338 15 ## 4 40.437 40.625 39.187 2.602727 -0.4243932 -191.5279 7896.840 15 ## 5 40.437 40.625 39.187 2.579562 -0.1809162 -199.7292 7891.172 15 ## 6 40.437 40.625 39.187 2.550577 0.1461197 -211.9402 7883.021 15 ## DIAG Flow_s Flow_r Txchg Tirga Tchopper Ts Tr CO2_. ## 1 2 506.541 663.349 25.4141 27.5924 30.0001 27.5408 27.5134 29.9284 ## 2 2 501.762 662.342 25.4212 27.5924 30.0001 27.5408 27.5134 29.7805 ## 3 2 514.578 663.419 25.4292 27.5924 30.0000 27.5408 27.5134 29.5901 ## 4 2 498.758 662.436 25.4351 27.5924 30.0000 27.5408 27.5134 29.3941 ## 5 2 522.239 662.835 25.4384 27.5924 30.0000 27.5408 27.5134 29.2541 ## 6 2 504.517 662.881 25.4413 27.5924 30.0000 27.5408 27.5134 29.0845 ## Desiccant_. Humidifier_. Heatx_setpoint CO2_r_setpoint H2O_r_setpoint ## 1 43.693 0 25.4324 460.667 17.2388 ## 2 43.693 0 25.4324 460.667 17.2388 ## 3 43.693 0 25.4390 455.833 17.2388 ## 4 43.693 0 25.4390 450.667 17.2388 ## 5 43.693 0 25.4390 450.667 17.2388 ## 6 43.693 0 25.4491 445.833 17.2388 ## SS_s SS_r MatchH2O MatchCO2 MatchValveR MatchValveS ## 1 100.741 101.259 -0.031 5.465 100 100 ## 2 100.741 101.260 -0.031 5.465 100 100 ## 3 100.742 101.261 -0.031 5.465 100 100 ## 4 100.742 101.261 -0.031 5.465 100 100 ## 5 100.742 101.261 -0.031 5.465 100 100 ## 6 100.743 101.261 -0.031 5.465 100 100 这里先不谈其他，比较一下代码的差别, read_csv 读取同样的我这一个文件，多了一个 local = locale(encoding = \"latin1\") 的设定，看上去复杂了，实际上多数情况是无需的，但我们这个文件内有希腊字母等存在，如果不实用就会提示 input string 1 is invalid in this locale 相关报错，在此不过多解释，相信大家看了下面关于 locale 的一个简单解释就明白了： locale 3.1 属性的差别 我们看一下读取的数据的类有无差别： library(tidyverse) df1 &lt;- read_csv(&quot;./data/aci_ex.csv&quot;, local = locale(encoding = &quot;latin1&quot;)) ## Parsed with column specification: ## cols( ## .default = col_double(), ## date = col_datetime(format = &quot;&quot;), ## LightAdaptedID = col_character(), ## DarkPulseID = col_character() ## ) ## See spec(...) for full column specifications. df2 &lt;- read.csv(&quot;./data/aci_ex.csv&quot;) class(df1) ## [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; class(df2) ## [1] &quot;data.frame&quot; 除了 data.frame 的属性外，readr 除了常规的 data.frame 的类外，还多了几个，它属于 tibble 的属性，我们不多说，看一下 Hadley 大神对 tibble 的解释： Definition 3.1 (tibble definition) Tibbles are a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not. tibble 不同于 dataframe 的地方主要在打印和构造子数据集时： Tibbles 只显示前 10 行数据，但会显示所有列，因此大数据时比较方便，此外，除了显示列名外，他还回显示数据类型。 此外，使用 “$” 构造子集时 tibble 要求严格的变量名称。例如： #dataframe df &lt;- head(iris) #tibble tf &lt;- as_tibble(iris) # return results even with wrong name df$Sepal.Leng ## [1] 5.1 4.9 4.7 4.6 5.0 5.4 # error tf$Sepal.Leng ## Warning: Unknown or uninitialised column: &#39;Sepal.Leng&#39;. ## NULL Tibbles 仍然可以使用 [ 和 [[: [ 返回的是另一个 tibble, 而 [[ 返回的是一个向量，不在需要 drop = FALSE! class(iris[ , 1, drop = FALSE]) ## [1] &quot;data.frame&quot; class(as_data_frame(iris)[ , 1]) ## Warning: `as_data_frame()` is deprecated, use `as_tibble()` (but mind the new semantics). ## This warning is displayed once per session. ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; 注意：tibble 和 dataframe 还有一个重要区别是 tibble 不会将字符转换为因子，相当于使用 stringsAsFactors = FALSE 3.2 数据类型的解析 前面已经提到了，tibble 会自动解析数据的类型，但是这个类型是怎么解析的，解析错误怎么办，这就不得不提 parse_* 一系列函数了： str(parse_logical(c(&quot;TRUE&quot;, &quot;FALSE&quot;, &quot;NA&quot;))) ## logi [1:3] TRUE FALSE NA str(parse_integer(c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;))) ## int [1:3] 1 2 3 str(parse_date(c(&quot;2011-01-01&quot;, &quot;1997-10-14&quot;))) ## Date[1:2], format: &quot;2011-01-01&quot; &quot;1997-10-14&quot; 需要注意的是，如果解析失败，在输出时会是 NA，这就是为什么一些他别乱的数据会出现大量 NA 的原因： x &lt;- parse_integer(c(&quot;123&quot;, &quot;456&quot;, &quot;abc&quot;, &quot;123.321&quot;)) ## Warning: 2 parsing failures. ## row col expected actual ## 3 -- an integer abc ## 4 -- no trailing characters .321 x ## [1] 123 456 NA NA ## attr(,&quot;problems&quot;) ## # A tibble: 2 x 4 ## row col expected actual ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 3 NA an integer abc ## 2 4 NA no trailing characters .321 parse_* 有八个函数，他们遵循相同的语法结构，其中： parse_logical 与 parse_integer 类型非常简单，不会出错（数据里面掺杂了其他类型那属于人的错误或机器的错误）。 parse_double 则是严格的解析方式，必须是浮点类型的2，而 parse_number 则相对宽松，我们的问题主要出现在万一你分析的是德国等欧洲国家的数据，反正我第一次看到德国人的发票愣是看了半天才想明白价格。 parse_character 则是非常容易解析的类型，其复杂也在编码上，对于我们来讲，多数情况是不大可能出错的。 parse_factor 功能是创建分类变量，对于我们实验数据时用的非常多的。 parse_datetime, parse_date, parse_time 则是专门解析时间日期的，也是最复杂的，例如常见的英美仪器时间，ISO 时间以及 CR1000 所采用的 julia day 和 LI-6800 所采用的 unix time 或者叫做 POSIX time 或 Epoch Time。 鉴于实际难度和我少打字的原则3，后面我只简单介绍因子类和时间日期类。 3.2.1 因子类型 因子是 R 中用来创建分类数据的，例如 fitacis 用来批量处理不同处理或小区的光合数据，group 参数用的就是 factor 类型，当然，dataframe 格式自动帮我们完成了从 character 到 factor 的转换。当然，需要注意，factor 是有不同水平的，如同我们实验有处理，处理也要分不同的水平，我觉得这个角度理解 factor 非常实在，尤其是对于 农学、林学、生态背景的我们。如果数据里的水平是你后加的，加的时候漏掉了一个水平，那后果也不是很严重，就是分析不能继续，直到你找到错误： fertilizer &lt;- c(&quot;N&quot;, &quot;P&quot;, &quot;K&quot;,&quot;CK&quot;) parse_factor(c(&quot;N&quot;, &quot;P&quot;, &quot;K&quot;, &quot;CK&quot;, &quot;NPK&quot;), levels = fertilizer) ## Warning: 1 parsing failure. ## row col expected actual ## 5 -- value in level set NPK ## [1] N P K CK &lt;NA&gt; ## attr(,&quot;problems&quot;) ## # A tibble: 1 x 4 ## row col expected actual ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 5 NA value in level set NPK ## Levels: N P K CK 如果不幸漏掉了 “NPK” 这个处理，那么这个解析就会报错，无法完成，直到你在 fertilizer 内增加了复合肥4这个处理。 如果更不幸的事情发生了，你排查不到错误，无法将字符转换为因子类型，那么无需着急，我们后面还有其他工具，此时就让他们作为字符类型好了。 3.2.2 时间与日期 parse_datetime 默认解析的方式非常符合中国人的习惯，采用的是 ISO8601, 年月日及时间的方式，如同我最开始数据导入章节 3 举例时，read_csv 十分准确的识别了这个时间。 对于仪器中长用的时间类型，无非是 ISO08601 的标准日期，julian day 日期格式5，POSIX 时间6，以及欧美时间的格式，处理起来各不相同。这里对时间日期的处理采用 lubridate 来处理： library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following object is masked from &#39;package:base&#39;: ## ## date 3.2.2.1 ISO08601 时间及欧美时间的处理 这种时间日期的导入绝大部分能够正常识别，如果不能可以以字符型导入，然后进行解析: ymd_hms(&quot;2019-02-25 21:28:59&quot;) ## [1] &quot;2019-02-25 21:28:59 UTC&quot; mdy_hms(&quot;02-25-2019 21:28:59&quot;) ## [1] &quot;2019-02-25 21:28:59 UTC&quot; 3.3 Julian day 的转换 这个日期尽管看起来奇怪，但是要转换比较容易，base 包即可以。不过我们需要指定起始日期，不然仪器不知道怎么去做，当然，我们既然使用了 lubridate，它也有对应的函数： jday &lt;- c(1, 8, 20, 370) as_date(jday, origin=&quot;2018-01-01&quot;) ## [1] &quot;2018-01-02&quot; &quot;2018-01-09&quot; &quot;2018-01-21&quot; &quot;2019-01-06&quot; 这样就一下子搞定了我们的茎流数据的日期了。 3.3.0.1 POSIX 日期的转换 这个对于广大程序员来讲非常熟悉，但是对于我们生态环境行业来讲，是对人类很不友好的数据格式，但对于转换来讲,其实也非常简单,如果是有具体的时间，则表示以秒为单位计算，我们可以使用 as_datetime： epoch &lt;- c(1, 100, 2.1e+8) as_datetime(epoch) ## [1] &quot;1970-01-01 00:00:01 UTC&quot; &quot;1970-01-01 00:01:40 UTC&quot; ## [3] &quot;1976-08-27 13:20:00 UTC&quot; 3.4 常用 package 介绍 3.5 readr 包核心函数 我们常用的函数，借用 readr cheetsheet 来展示一下其主要用途： 图 3.1: readr 常用函数图解 3.6 readxl 对于读取 excel 格式的文件，一个函数就足够了： read_excel(path, sheet = NULL, range = NULL, col_names = TRUE, col_types = NULL, na = &quot;&quot;, trim_ws = TRUE, skip = 0, n_max = Inf, guess_max = min(1000, n_max), progress = readxl_progress(), .name_repair = &quot;unique&quot;) 自动判断 xls 或者 xlsx 格式来读取，遗憾的是，对于 LI-6400 的 xls 格式，它无能为力，无法读取，对于 LI-6800 的 xlsx 格式，不能正确的识别其公式，因为表头太多了，如果单纯只有测量数据的文件，它是没有问题的。 3.7 二进制文件 此处的二进制文件特指 R 原生的 rds 格式和最新的 feather 格式。他们都是轻量级的数据格式，feather 的优势是能够保存 tibble 数据解析的数据格式。至于读取相应类型的数据，则非常容易，对于我们来讲，其读取的意义在于我们将大量的其他格式数据存储为这两种格式后，进行读取，直接是这两种格式的仪器数据，目前来讲应该还没有。 readRDS(file, refhook = NULL) read_feather(path, columns = NULL) 3.8 rio–万能的瑞士军刀 前面几节介绍读取数据的函数是走马观花，蜻蜓点水的方式，不是故意偷懒，是因为本节内容才是重点，之所以标题将 rio 称之为万能的瑞士军刀，是因为其功能决定的： rio 实际上属于作者对 data.table、haven、readxl 等一系列软件包相关函数的打包，然后将格式的识别自动化，减少了我们的工作量，主要特点为： 使用扩展名来识别文件类型，减少人工输入的工作量，若是格式无法识别，则可以通过指定 format 来导入。 reader 用来处理常见的文本数据，无需指定特定的数据类型。 io 处理自定义数据格式。 ImportExport 集中于处理 excel、SPSS 等二进制文件并提供 shiny 的界面。 SchemaOnRead 则是通过一系列的迭代找到最合适的读取数据的方法。 值得一提的是，rio 尽管使用了 base 函数读取数据，但他从不将字符串当作因子类型处理，遵循 tidyverse 的原则。我们通过举例来完成相关函数的介绍： 3.8.1 安装 rio 安装略微不同于其他软件包，安装好之后，我们最好通过 install_formats() 命令进一步安装其他缺失的软件包，以进一步获得完整的数据格式的支持。 install.packages(&quot;rio&quot;) library(rio) install_formats() 3.8.2 数据的读取 读取数据时，我们可以忘掉原来那一系列函数，只需记得 import 即可： 3.8.2.1 读取 csv 格式数据 library(rio) aci &lt;- import(&quot;./data/aci6800.csv&quot;) 当然，这不足以显示 rio 的优势，因为这种简单格式对 read.csv 也不费力，那我们来点高级的，我这里有一个文件夹，里面放了 4 个 csv 文件： library(plantecophys) paths &lt;- Sys.glob(&quot;./data/multi_csv/*.csv&quot;) all_data &lt;- import_list(paths) fits &lt;- lapply(all_data, fitaci, fitmethod = &quot;bilinear&quot;) fits$aci4$pars ## Estimate Std. Error ## Vcmax 49.3787547 3.4815555 ## Jmax 128.5546403 NA ## Rd 0.3828608 0.4697008 有了 import_list，是不是连 fitacis 也显得多余了？我们无需用 lapply 导入所有数据再合并数据，并加入一列 factor 来区分我们的数据，省时省力，怎么能叫人不喜欢呢？ 3.8.2.2 读取 excel 格式数据 对于 excel 格式的数据读取，我们只需要正确的输入文件名和扩展名即可，不用管它是 xls 还是 xlsx 的格式。下面我有一个 叫做 aci01.xls 的文件，里面有多个 sheet，我们来读取一个叫做 aci2 的 sheet 内的数据： aci2 &lt;- import(&quot;./data/aci01.xls&quot;, sheet = &quot;aci2&quot;) 当然，多数时候我们的数据都是只有一个 sheet 的，或者像我一样不喜欢把所有数据都 分 sheet 放在一个文件的人也很多，所以我们很多时候是不需要这个 sheet = \"aci2 这个参数的，但对于某些仪器，一次导出多个 excel 文件也不时很实用，我们需要的数据恰恰又放在了某个 sheet 中，就很实用了，例如 METERS 的仪器喜欢这么做，拿 SATURO 双水头来讲，如果我们需要原始数据来做处理分析： raw_data &lt;- import(&quot;./data/clay1.xlsx&quot;, sheet = &quot;Raw Data&quot;) knitr::kable(head(raw_data)) Record ID Time (min) Water Level (cm) Pressure (cm) Flux (cm/s) Volume (mL/s) 4 1 4.35 5.506 0.00122 0.2225459 5 2 5.07 4.094 0.00000 0.0000000 6 3 4.97 4.838 0.00323 0.5891995 7 4 4.94 5.605 0.00616 1.1236745 8 5 4.95 5.264 0.00476 0.8682939 9 6 4.94 5.424 0.00555 1.0124015 那如果就是喜欢把所有数据放到一个文件里怎么办，答案我们已经见过： all_aci &lt;- import_list(&quot;./data/aci01.xls&quot;) attributes(all_aci) ## $names ## [1] &quot;aci1&quot; &quot;aci2&quot; &quot;aci3&quot; 如果要进一步处理，参考 3.8.2.1 内容。 3.8.2.3 文本文件的处理 以上两种格式常见，但有时我们会遇到其他文本格式的数据，即虽然数据为文本格式，但都带有其他特别的后缀，例如 CR1000 的数据，如果你曾经用文本编辑器打开过 CR1000 的数据，很容易看到他是用逗号分隔的，那么我们看一下 rio 的表现： crdata &lt;- import(&quot;./data/weather.dat&quot;, format = &quot;,&quot;, skip = 1) knitr::kable(head(crdata)) TIMESTAMP RECORD Batt_volt_Min PTemp_C_Max SR_Wpm2_Avg PAR_umolpm2s_Avg NR_uncorrect_Wpm2_Avg NR_correct_Wpm2_Avg Soil_hf_Wpm2_Avg RG_mm_Tot TCAir_C_Avg RH_Pcent_Avg WS_mps_Avg WS_gust_mps WD_360_Avg TCSoil_C_Avg VWC_Avg Eb_Avg TS RN Min Max Avg Avg Avg Avg Avg Tot Avg Avg Avg Smp Avg Avg Avg Avg 2018-12-14 16:30:00 0 12.84 6.242 28.16 -56.77 -3.978 -3.978 19.54 0 3.148 40.87 NAN 0 0 0 NAN NAN 2018-12-14 17:00:00 1 12.86 4.299 17.75 64.06 -3.7 -3.7 56.13 2 1.623 50.8 0.269 0.13 204.1 0.059 -0.006 1.761 2018-12-14 17:30:00 2 12.88 -0.095 0.713 1.433 -3.044 -3.044 26.31 0 -0.695 71.51 0.195 0.3 254.4 -2.945 -0.017 1.28 2018-12-14 18:00:00 3 12.87 -3.385 -0.006 -0.018 -2.633 -2.633 29.05 0 -1.993 82.6 0.193 0.12 231.9 -3.443 -0.015 1.331 不太好的情况是，我们导 入了表头下 面的一行不需要的 内容，这个我们 先忽略，后面再 处理。 对于 LI-840 等 txt 数据，喜欢用空格分列，使用 rio 的效果非常好，直接自动删除了第一个头文件的日期，读取了我测量的数据： li840 &lt;- import(&quot;./data/li840.txt&quot;) knitr::kable(head(li840)) Date(Y-M-D) Time(H:M:S) CO2(ppm) H2O(ppt) H2O(C) Cell_Temperature(C) Cell_Pressure(kPa) CO2_Absorption H2O_Absorption 2017-06-14 11:07:19 707.61 18.76 16.49 52.19 100.37 0.1328 0.1067 2017-06-14 11:07:20 707.62 18.76 16.49 52.19 100.37 0.1328 0.1067 2017-06-14 11:07:21 707.78 18.77 16.49 52.19 100.35 0.1328 0.1067 2017-06-14 11:07:22 707.87 18.77 16.50 52.19 100.35 0.1328 0.1067 2017-06-14 11:07:23 707.78 18.77 16.50 52.19 100.35 0.1328 0.1067 2017-06-14 11:07:24 707.84 18.77 16.50 52.19 100.35 0.1328 0.1067 R 内没有 float↩ 有时候我会把省掉的补充上，如果后面遇到不得不详细讲的内容时。↩ 农学的童鞋都理解吧，我记得我当时生物统计教材将处理和水平时就用的不同施肥，虽然当时没学好，这么“土的掉渣”的例子我还记得。↩ 一年当中的第几天↩ unix时间，从1970年1月1日开始计算，距离它的秒来几时↩ "],
["rioexport.html", "第 4 章 使用 rio 导出数据 4.1 数据的转换", " 第 4 章 使用 rio 导出数据 与导入数据函数相比，基本上 R 内都有其相对应的导出数据的函数，但为了节省版面，外加这两天培训实在有点累，我们直接进入正题，还是直接进入 “Swiss-Army Knife” 的解决方案： 如果是导入的数据，那么非常容易解决，例如刚刚导入的 CR1000 的数据： exprot(crdata, &quot;./data/crdata.csv&quot;) 如果我想导出多个 dataframe，也非常简单： exprot(all_aci, &quot;./data/all_aci.xlsx&quot;) 也就是第一个参数为多个 dataframe 组成的 list，然后可以将单个 dataframe 导入 excel 不同的 sheet 内。 4.1 数据的转换 rio 数据的转换其实并不奇怪，他是先讲数据导入，然后再导出，目的在于导出，所以我们将它放在了导出数据的内容内。 convert(&quot;./data/aci.csv&quot;, &quot;./data/aci.feather&quot;) 注：转换为 feather 格式，并非耍酷的需要，主要目的有三，一是该格式数据轻量级的，占地方小，二是他是 R 和 python 通用的格式，三是这是二进制格式，文本打开一堆乱码，有一定安全意义，次要目的是耍酷。 "],
["tidydata.html", "第 5 章 数据的转换 5.1 使用 tidyr 清洁数据", " 第 5 章 数据的转换 首先，tidyverse 数据的核心为 “tidy data”，所有软件都是要根据这个核心来进行数据处理，或者是将非 “tidy data” 转换为 “tidy data”，那么什么是 “tidy data” 呢？ Wickham (2014) 对其进行了详细的阐述，核心观点可以参考图 5.1: url &lt;- &quot;https://d33wubrfki0l68.cloudfront.net/6f1ddb544fc5c69a2478e444ab8112fb0eea23f8/91adc/images/tidy-1.png&quot; knitr::include_graphics(&quot;url&quot;) 图 5.1: 图解“tidy data” 用三句话概括： 每个变量必须单独成列。 每个观察值必须单独成行。 每个数值必须具有单独的单元格。 拿整理好的光合数据举例，每个测量参数，如 A，Ci 等，都具有单独的列，每个 obs 列代表了一次测量，每次测量都是单独的一行，对应了各个测量的参数，每个单元格只有一个数值，不存在这样表示的单元格, 例如 表头为 “A/Ci”，观测值为 “12/200”。 这样做的优势也是显而易见的： 所有数据的存储结构都是一致的，我们调用非常方便，拿到数据后不需要思考，直接调用即可。最重要的，他是 “tidyverse” 以及多数的软件包所支持的格式。 R 是原生的支持向量化操作的软件，将每个变量单独成列，也就是每个变量都是同一数据类型，本质上就是向量，这样 R 内置的函数都支持这些类型的数据的处理。 5.1 使用 tidyr 清洁数据 tidyr 核心函数有三个，分别是 gather(), separate() 以及 spread()，主要目的是对数据进行拆分，合并等清洁操作，单独使用是可行的，但最好结合 dplyr 来进行操作，会大大的解放我们的生产力，这里我们先不对 dplyr 的内容进行介绍，后面 6 再详细介绍。 5.1.1 gather 用于合并数据集 gather 用于将多列数据合并为一列，视觉上看上去数据由宽变长，这是它的一个特征，例如我有这样两个数据，按月份测量了光合速率 (表 5.1) 和蒸腾速率 (表 5.2)： messa &lt;- readRDS(&quot;./data/messa.RDS&quot;) messe &lt;- readRDS(&quot;./data/messe.RDS&quot;) 表 5.1: 5-6月光合数据 species May Jun wheat 20.61053 30.90637 wheat 20.18174 29.39688 corn 17.94135 14.65996 corn 17.81593 14.06745 表 5.2: 5-6月蒸腾数据 species May Jun wheat 0.0303354 0.0501694 wheat 0.0299274 0.0501956 corn 0.0348524 0.0039654 corn 0.0350129 0.0039123 这个数据记录的最大问题是，后面两列都是同一变量，按照原则，二者应为同一列，而不是分为两列，需要合并，tidyr 都是 tidyverse 的核心包，此时我们只用 tidyr： library(tidyr) tidya_data &lt;- messa %&gt;% gather(May, Jun, key = &quot;month&quot;, value = &quot;A&quot;) tidye_data &lt;- messe %&gt;% gather(May, Jun, key = &quot;month&quot;, value = &quot;E&quot;) 清洁后数据分别见表 5.1 和表 5.2。 表 5.3: 清洁后5-6月光合数据 species month A wheat May 20.61053 wheat May 20.18174 corn May 17.94135 corn May 17.81593 wheat Jun 30.90637 wheat Jun 29.39688 corn Jun 14.65996 corn Jun 14.06745 表 5.4: 清洁后5-6月蒸腾数据 species month E wheat May 0.0303354 wheat May 0.0299274 corn May 0.0348524 corn May 0.0350129 wheat Jun 0.0501694 wheat Jun 0.0501956 corn Jun 0.0039654 corn Jun 0.0039123 先简单介绍一下 gather 怎么实现上述清洁：key 指的是我们原来数据中表头的名字，value 指的是原来数据中的测量值，在本例中 key 为我们不同月份，value 是我们每月测量的光合速率的值。 如果你留意到了 “%&gt;”，那么恭喜你，你注意到了 tidyverse 软件包所支持的一种非常直观的语法，我们称之为管道（pipes），它来自 magrittr，跟 linux 语法中的管道意思类似，可以很方便的讲我们符号之前的变量传递给后面的变量，例如： library(magrittr) ## ## Attaching package: &#39;magrittr&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## set_names ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract x = 1:1000 z = sum((mean(diff(x))), x) z ## [1] 500501 z &lt;- x %&gt;% diff() %&gt;% mean() %&gt;% sum(x) z ## [1] 500501 当然，在此处的优势不是特别明显,但单从视觉上来看，无疑使用管道符号更为直观，当然，当你知道可以通过 ctr+shift+M 可以直接输入 %&gt;% 时，无疑你会从痛苦和疑惑中解脱。其他优势我们不介绍，后面使用时会很清晰的看到。 5.1.2 spread 用于展开数据集 与 gather 相对应，spread 用于将数据展开，例如我有如表 5.5 的数据 messc： messc &lt;- readRDS(&quot;./data/messc.RDS&quot;) 表 5.5: 5-6月光合蒸腾数据 species month type measure wheat May A 20.3393647 wheat May E 0.0299543 wheat Jun A 20.0542329 wheat Jun E 0.0398827 corn May A 30.2665086 corn May E 0.0498585 corn Jun A 30.1753295 corn Jun E 0.0549767 很明显，光合和蒸腾属于两个变量，不应放在一起，spread 使用方式同 gather 类似，其中 key 指的是 type，value 只的是测量值 measure： tidyc_data &lt;- messc %&gt;% spread(key=type,value = measure) (#tab:tidyc_data)清洁后的5-6月光合蒸腾数据 species month A E corn Jun 30.17533 0.0549767 corn May 30.26651 0.0498585 wheat Jun 20.05423 0.0398827 wheat May 20.33936 0.0299543 5.1.3 separate 用于单列数据的分离 对于一些手动记录的调查数据，通常存在的问题就是本该分成两列的数据放在了一列，用了一些符号和空格隔开，例如表 5.6 数据 crsep &lt;- readRDS(&quot;./data/crseparate.RDS&quot;) 表 5.6: 未分列的数据 date volt_ptemp 2018/5/19 11:00 14/26 2018/5/19 11:30 14/27 2018/5/19 12:00 14/28 2018/5/19 12:30 14/28 2018/5/19 13:00 14/28 2018/5/19 13:30 14/28 时间和日期如果是分开的，对于我们处理起来比较方便，机箱的电池电压和温度同样，不应在一列： crnor &lt;- crsep %&gt;% separate(date, into = c(&quot;date&quot;, &quot;time&quot;), sep = &quot; &quot;) %&gt;% separate(volt_ptemp, into = c(&quot;batt_v&quot;, &quot;ptemp&quot;)) 注意，我们只有第一次分列使用了 sep= 参数，原因是 separate 默认使用非字母数字分隔。日期和时间中存在了非字母数字的符号，所以我们制定使用空格分列，而对于后面的数据，没有其他非字母数字的符号存在，整理后数据如 5.7 所示。 表 5.7: 分列处理后的数据 date time batt_v ptemp 2018/5/19 11:00 14 26 2018/5/19 11:30 14 27 2018/5/19 12:00 14 28 2018/5/19 12:30 14 28 2018/5/19 13:00 14 28 2018/5/19 13:30 14 28 此时我们需要注意的是，如果我们看一下整理好后的数据类型： str(crnor) ## &#39;data.frame&#39;: 15 obs. of 4 variables: ## $ date : chr &quot;2018/5/19&quot; &quot;2018/5/19&quot; &quot;2018/5/19&quot; &quot;2018/5/19&quot; ... ## $ time : chr &quot;11:00&quot; &quot;11:30&quot; &quot;12:00&quot; &quot;12:30&quot; ... ## $ batt_v: chr &quot;14&quot; &quot;14&quot; &quot;14&quot; &quot;14&quot; ... ## $ ptemp : chr &quot;26&quot; &quot;27&quot; &quot;28&quot; &quot;28&quot; ... 我们发现这些类型的识别是不符合实际的，但我们可以使用 separate 方便的进行转换： crnor &lt;- crsep %&gt;% separate(date, into = c(&quot;date&quot;, &quot;time&quot;), sep = &quot; &quot;, convert = TRUE) %&gt;% separate(volt_ptemp, into = c(&quot;batt_v&quot;, &quot;ptemp&quot;), convert = TRUE) str(crnor) ## &#39;data.frame&#39;: 15 obs. of 4 variables: ## $ date : chr &quot;2018/5/19&quot; &quot;2018/5/19&quot; &quot;2018/5/19&quot; &quot;2018/5/19&quot; ... ## $ time : chr &quot;11:00&quot; &quot;11:30&quot; &quot;12:00&quot; &quot;12:30&quot; ... ## $ batt_v: int 14 14 14 14 14 14 14 14 14 14 ... ## $ ptemp : int 26 27 28 28 28 28 29 30 31 31 ... 时间和日期的格式还是不对，不过这不影响大局，格式转换也不是 tidyr 的所擅长的内容，我们后面介绍 dplyr 时再介绍。 还有就是 sep 除了可以设置字符类型外，还可以设置为整数，这样可以方便的按给定的位数分隔字符，例如下面的时间日期写法，如果放在一列，是很不方便的，但它没有分隔符号： messdt &lt;- tibble::tibble(dt=c(&quot;198508181311&quot;, &quot;198609191412&quot;, &quot;198710101513&quot; )) messdt %&gt;% separate(dt, into = c(&quot;date&quot;, &quot;time&quot;), sep = &quot;8&quot;, convert = TRUE) ## Warning: Expected 2 pieces. Additional pieces discarded in 1 rows [1]. ## # A tibble: 3 x 2 ## date time ## &lt;int&gt; &lt;int&gt; ## 1 19 50 ## 2 19 609191412 ## 3 19 710101513 messdt %&gt;% separate(dt, into = c(&quot;date&quot;, &quot;time&quot;), sep = 8, convert = TRUE) ## # A tibble: 3 x 2 ## date time ## &lt;int&gt; &lt;int&gt; ## 1 19850818 1311 ## 2 19860919 1412 ## 3 19871010 1513 上面的例子告诉我们，使用 sep 分隔数据时，千万注意利用位置分列时，不要加引号，因为它把 “8” 当作了分隔符号。 5.1.4 unite 整合不同的列 unite 作用和 separate 恰恰相反，在实际应用过程中不是很常见，但有句话说得好，“书到用时方恨少”，类似的，函数到用时，如果没有，也会给我们带来额外的工作量，例如我有三个小区，每个小区测量三个植株，每个植株测量三个叶片的光合速率，如表 5.8 所示： sampleid &lt;- data.frame( plot = rep(1:3, each = 3, times = 3 ), plant = rep(1:3, each = 9), leaf = rep(1:3, 9), A = rnorm(27, 20, 2 ) ) 表 5.8: 整合前数据的样式（未完全显示） plot plant leaf A 1 1 1 19.43561 1 1 2 17.93471 1 1 3 20.94253 2 1 1 15.42295 2 1 2 20.61723 2 1 3 18.77323 但我们在处理数据的时候，通常只需要一列来区分不同来源的参数，整理完成如表 @ref(tab:tidy_smp) 的样式更符合 tidy data 的要求： tidy_smp &lt;- sampleid %&gt;% unite(treatment, plot, plant, leaf, sep = &quot;-&quot;) (#tab:tidy_smp)整合三列后数据的样式（未完全显示） treatment A 1-1-1 19.43561 1-1-2 17.93471 1-1-3 20.94253 2-1-1 15.42295 2-1-2 20.61723 2-1-3 18.77323 整合时可以自定义连接的符号，例如我们做不同处理通常喜欢用上面的显示方式，当然如果特殊需求不喜欢用符号，可以直接设置为 sep = \"\"。 参考文献 "],
["dplyr.html", "第 6 章 dplyr–数据操作的语法", " 第 6 章 dplyr–数据操作的语法 "],
["todo.html", "第 7 章 todo 7.1 data transformation 7.2 data visulization 7.3 model r 7.4 public", " 第 7 章 todo 7.1 data transformation 7.2 data visulization 7.3 model r 7.4 public "],
["references.html", "参考文献", " 参考文献 "]
]
